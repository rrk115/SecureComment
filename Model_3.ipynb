{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, Dropout, LSTM, SimpleRNN, GRU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "train_csv = 'C:/Users/Kowshik Rayani/Downloads/train.csv/train.csv'\n",
    "train_df = pd.read_csv(train_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Preperation\n",
    "* read the labels and convert into one-class labels\n",
    "* we will focus on 2 class problem: toxic and non toxic comments\n",
    "* we will label all different types of toxic comments into same category of toxic label:\n",
    "    * 0 for toxic comment\n",
    "    * 1 for non-toxic comments\n",
    "* later we can explore how to make it multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each toxic class is labelled as 1\n",
    "toxic_row_sums = train_df.iloc[:,2:].sum(axis=1)\n",
    "# if sum of toxic class is 0 then it is a clean comment\n",
    "train_df['clean'] = (toxic_row_sums==0)\n",
    "# Input Data\n",
    "train_texts = train_df['comment_text']\n",
    "# Output Label\n",
    "train_labels = train_df['clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing : Tokenization\n",
    "Now we have training data in two separate dataframe columns (arrays/list): an ordered array consisting of comments (input for the network) and another array consisting of class lables in same order (output of the network).\n",
    "\n",
    "We have to transform this data into network input format and output format. This step is called pre-processing.  \n",
    "Steps of pre-processing:\n",
    "\n",
    "1. Tokenize the text into words\n",
    "2. Assign each word a dimension\n",
    "\n",
    "\n",
    "To accompolish step 1 and 2 we will use inbuilt __Tokenizer__ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688, 75, 1, 126, 130, 177, 29, 672, 4511, 1116, 86, 331, 51, 2278, 50, 6864, 15, 60, 2756, 148, 7, 2937, 34, 117, 1221, 2825, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 143, 73, 3462, 89, 3085, 4583, 2273, 985]\n",
      "Found 210337 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "print(sequences[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and Padding for Embedding\n",
    "Now once we have the tokens and each token(word) has a dimension assigned to it, we will do following steps to create word embeddings  \n",
    "\n",
    "3. use this dimension assignments to define embedding for individual word\n",
    "4. use word embedding to create word vector for a comment\n",
    "\n",
    "\n",
    "We will use a specific type of network layer for this, which is called __Embedding Layer__. The above generated tokens (sequence of number) will go as input to Embedding layer, which will output word embeddings as output to next layer.  \n",
    "\n",
    "Input and Output of Neural Network are done is batches. A batch is a group of input data which are fed together to the network. As the network can process individual data element in parallel, the training will be faster.\n",
    "\n",
    "In case of Embedding Layer, Inpupt and Output in a batch can be seen as follows:  \n",
    "\n",
    "   **Input**: 2D tensor of integers, of shape (# seq. samples in particular batch, sequence_length), where each entry is a sequence of integers (output of above code).  \n",
    "   **Output**: 3D floating-point tensor of shape (# seq. samples in particula patch, sequence_length, embedding_dimensionality).  \n",
    "\n",
    "Sequence length can be variable per batch. But in a single batch sequence length will be same for all sequences.  \n",
    "\n",
    "So from data we have to create batches of sequence of similar length and to do that we have to pad or truncate each sequence to have same sequence length. And we can use each batch as a training input for embedding layer.  \n",
    "\n",
    "For sample case: we take 10k sequence from 160k for training in a single batch. And take max sequence length of 20 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "training_sequences = sequences[:10000]\n",
    "training_labels = train_labels[:10000]\n",
    "seq_max_len = 20\n",
    "# training padded sequences\n",
    "train_seq_pad = preprocessing.sequence.pad_sequences(sequences=training_sequences, maxlen=seq_max_len)\n",
    "\n",
    "# testing padded sequences\n",
    "testing_sequences = sequences[10000:11000]\n",
    "testing_labels = train_labels[10000:11000]\n",
    "test_seq_pad = preprocessing.sequence.pad_sequences(sequences=testing_sequences, maxlen=seq_max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "embedding_dim = 16\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(10000, embedding_dim, input_length=seq_max_len))\n",
    "model_3.add(LSTM(32))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(1, activation='sigmoid'))  \n",
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 16)            160000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                6272      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 166305 (649.63 KB)\n",
      "Trainable params: 166305 (649.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 39s 67ms/step - loss: 0.2948 - accuracy: 0.9014 - val_loss: 0.2120 - val_accuracy: 0.9295\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 38s 76ms/step - loss: 0.1535 - accuracy: 0.9480 - val_loss: 0.1988 - val_accuracy: 0.9360\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 55s 110ms/step - loss: 0.0872 - accuracy: 0.9703 - val_loss: 0.2248 - val_accuracy: 0.9290\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 107s 213ms/step - loss: 0.0479 - accuracy: 0.9856 - val_loss: 0.2715 - val_accuracy: 0.9350\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 74s 149ms/step - loss: 0.0303 - accuracy: 0.9918 - val_loss: 0.3352 - val_accuracy: 0.9275\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 69s 138ms/step - loss: 0.0184 - accuracy: 0.9949 - val_loss: 0.4304 - val_accuracy: 0.9305\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 76s 153ms/step - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.4195 - val_accuracy: 0.9300\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 137s 274ms/step - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.3926 - val_accuracy: 0.9225\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 98s 196ms/step - loss: 0.0078 - accuracy: 0.9974 - val_loss: 0.4166 - val_accuracy: 0.9385\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 154s 308ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.4666 - val_accuracy: 0.9260\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "history_3 = model_3.fit(train_seq_pad, np.asarray(training_labels), epochs=10, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'accuracy']\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4467 - accuracy: 0.9320\n",
      "Test Loss: 0.4467005431652069\n",
      "Test Accuracy: 93.19999814033508 %\n"
     ]
    }
   ],
   "source": [
    "print(model_3.metrics_names)\n",
    "evaluation_results = model_3.evaluate(x=test_seq_pad, y=np.asarray(testing_labels))\n",
    "print(f\"Test Loss: {evaluation_results[0]}\")\n",
    "print(f\"Test Accuracy: {evaluation_results[1]*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kowshik Rayani\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_3.save('toxic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model1 = load_model('toxic.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kohli is bad boy ']\n"
     ]
    }
   ],
   "source": [
    "#test_texts = [\"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.\"]\n",
    "test_texts = [\"kohli is bad boy \"]\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "test_seq_pad = preprocessing.sequence.pad_sequences(sequences=test_sequences, maxlen=seq_max_len)\n",
    "\n",
    "#predictions = model1.predict(test_seq_pad)\n",
    "print(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict(test_seq_pad, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "binary_predictions = (predictions > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "print(binary_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Toxic Comments: 100.0%\n",
      "Toxic Comments: 0.0%\n"
     ]
    }
   ],
   "source": [
    "num_non_toxic = np.sum(binary_predictions == 1)\n",
    "num_toxic = np.sum(binary_predictions == 0)\n",
    "total_comments = len(binary_predictions)\n",
    "\n",
    "non_toxic_percentage = (num_non_toxic / total_comments) * 100\n",
    "toxic_percentage = (num_toxic / total_comments) * 100\n",
    "\n",
    "print(f\"Non-Toxic Comments: {non_toxic_percentage}%\")\n",
    "print(f\"Toxic Comments: {toxic_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
